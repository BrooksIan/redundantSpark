"""
Example script demonstrating file-level deduplication using Spark.
This script uses the duplicate files generated by generate_duplicate_files.py
"""

from pyspark.sql import SparkSession
from deduplicate_spark import create_spark_session, file_level_deduplication
import os
import glob


def main():
    """Demonstrate file-level deduplication."""
    # Create Spark session
    spark = create_spark_session("FileDeduplicationExample")
    
    # Find all files in the duplicatefiles directory
    duplicate_files_dir = "data/duplicatefiles"
    
    if not os.path.exists(duplicate_files_dir):
        print(f"Directory {duplicate_files_dir} does not exist!")
        print("Please run: python generate_duplicate_files.py 25 0.9")
        return
    
    # Get all files
    file_paths = glob.glob(os.path.join(duplicate_files_dir, "*"))
    file_paths = [f for f in file_paths if os.path.isfile(f)]
    
    if not file_paths:
        print(f"No files found in {duplicate_files_dir}!")
        return
    
    print(f"Found {len(file_paths)} files to analyze\n")
    
    # Perform file-level deduplication
    unique_files, duplicate_groups = file_level_deduplication(
        spark, 
        file_paths, 
        output_dir="data"
    )
    
    if unique_files is None:
        print("No files processed")
        return
    
    # Show results
    print("\n" + "="*70)
    print("FILE DEDUPLICATION RESULTS")
    print("="*70)
    
    unique_count = unique_files.count()
    duplicate_count = duplicate_groups.count() if duplicate_groups else 0
    
    print(f"\nTotal files analyzed: {len(file_paths)}")
    print(f"Unique files: {unique_count}")
    print(f"Duplicate groups: {duplicate_count}")
    
    if duplicate_count > 0:
        print("\nDuplicate file groups:")
        duplicate_groups.select(
            "content_hash",
            "duplicate_paths",
            "duplicate_count"
        ).show(truncate=False)
    
    print("\nUnique files (one per content hash):")
    unique_files.select(
        "file_name",
        "file_path",
        "file_size",
        "content_hash"
    ).show(truncate=False)
    
    print("="*70 + "\n")
    
    spark.stop()


if __name__ == "__main__":
    main()

