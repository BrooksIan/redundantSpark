{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Deduplication Lab - Getting Started\n",
        "\n",
        "Welcome to the Data Deduplication Lab! This notebook will help you get started with the lab exercises.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "- Understand different deduplication strategies\n",
        "- Be able to process large datasets efficiently with Spark\n",
        "- Know how to use approximate methods for memory-efficient operations\n",
        "- Understand file-level vs record-level deduplication\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Access to Cloudera AI Workbench\n",
        "- Basic Python knowledge\n",
        "- Familiarity with data processing concepts\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's verify that we have access to the necessary files and Spark is available.\n",
        "\n",
        "**Important**: Make sure you have uploaded the following files to your Cloudera AI Workbench project:\n",
        "- `deduplicate_spark.py` (in the project root)\n",
        "- `generate_dataset.py` (in the project root)\n",
        "- `bloom_filter_hyperloglog.py` (in the project root)\n",
        "- `bloom_filter_file_deduplication.py` (in the project root)\n",
        "\n",
        "The notebook will automatically add the project root to the Python path so these modules can be imported.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to Python path so we can import modules\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Find project root - try multiple methods\n",
        "current_dir = os.getcwd()\n",
        "project_root = None\n",
        "\n",
        "# Method 1: If we're in notebooks/, go up one level\n",
        "if 'notebooks' in current_dir:\n",
        "    project_root = os.path.dirname(current_dir)\n",
        "    print(f\"Found project root (from notebooks/): {project_root}\")\n",
        "# Method 2: Check if deduplicate_spark.py is in current directory\n",
        "elif os.path.exists(os.path.join(current_dir, 'deduplicate_spark.py')):\n",
        "    project_root = current_dir\n",
        "    print(f\"Found project root (deduplicate_spark.py in current dir): {project_root}\")\n",
        "# Method 3: Search up directories\n",
        "else:\n",
        "    test_dir = current_dir\n",
        "    for _ in range(5):  # Try up to 5 levels\n",
        "        if os.path.exists(os.path.join(test_dir, 'deduplicate_spark.py')):\n",
        "            project_root = test_dir\n",
        "            print(f\"Found project root (searched up): {project_root}\")\n",
        "            break\n",
        "        parent = os.path.dirname(test_dir)\n",
        "        if parent == test_dir:  # Reached filesystem root\n",
        "            break\n",
        "        test_dir = parent\n",
        "    \n",
        "    # Fallback to current directory\n",
        "    if project_root is None:\n",
        "        project_root = current_dir\n",
        "        print(f\"Using current directory as project root: {project_root}\")\n",
        "\n",
        "# Add project root to path if not already there\n",
        "if project_root and project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "    print(f\"✓ Added to Python path: {project_root}\")\n",
        "\n",
        "# Change to project root for file operations\n",
        "os.chdir(project_root)\n",
        "print(f\"✓ Changed working directory to: {project_root}\")\n",
        "\n",
        "# Check if Spark is available\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    print(\"✓ PySpark is available\")\n",
        "except ImportError:\n",
        "    print(\"✗ PySpark not found. Please ensure Spark is installed.\")\n",
        "\n",
        "# Check if our deduplication module is available\n",
        "try:\n",
        "    from deduplicate_spark import create_spark_session\n",
        "    print(\"✓ deduplicate_spark module is available\")\n",
        "except ImportError as e:\n",
        "    print(\"✗ deduplicate_spark module not found.\")\n",
        "    print(f\"  Current working directory: {os.getcwd()}\")\n",
        "    print(f\"  Project root used: {project_root}\")\n",
        "    print(f\"  Python path (first 3): {sys.path[:3]}\")\n",
        "    print(f\"  Error: {e}\")\n",
        "    print(\"\\n  Troubleshooting:\")\n",
        "    print(\"  1. Ensure deduplicate_spark.py is in the project root\")\n",
        "    print(\"  2. Check that the file is uploaded to Cloudera AI Workbench\")\n",
        "    print(\"  3. Verify the file is in the same directory as this notebook's parent\")\n",
        "\n",
        "# Check Python version\n",
        "print(f\"\\n✓ Python version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Spark Session\n",
        "\n",
        "The `create_spark_session()` function automatically detects if you're running in Cloudera AI Workbench and configures Spark appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deduplicate_spark import create_spark_session\n",
        "\n",
        "# Create Spark session (automatically configured for Cloudera)\n",
        "spark = create_spark_session(\"DeduplicationLab\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark master: {spark.sparkContext.master}\")\n",
        "print(\"✓ Spark session created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sample Data\n",
        "\n",
        "Let's generate a sample dataset with duplicates to work with. We'll create a CSV file with 1000 records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Ensure we're in project root (should already be set from previous cell)\n",
        "# Generate sample data in project root's data/ directory\n",
        "data_dir = os.path.join(project_root, \"data\")\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "    print(f\"✓ Created data directory: {data_dir}\")\n",
        "\n",
        "# Generate dataset with 1000 records\n",
        "output_file = os.path.join(data_dir, \"redundant_data.csv\")\n",
        "script_path = os.path.join(project_root, \"generate_dataset.py\")\n",
        "\n",
        "print(f\"Running: python {script_path} 1000 {output_file}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "result = subprocess.run(\n",
        "    [\"python\", script_path, \"1000\", output_file],\n",
        "    cwd=project_root,  # Ensure we run from project root\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"✓ Sample data generated successfully!\")\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "else:\n",
        "    print(\"✗ Error generating data:\")\n",
        "    print(result.stderr)\n",
        "    print(f\"\\nTroubleshooting:\")\n",
        "    print(f\"  - Script path: {script_path}\")\n",
        "    print(f\"  - Script exists: {os.path.exists(script_path)}\")\n",
        "    print(f\"  - Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Check if file was created\n",
        "if os.path.exists(output_file):\n",
        "    file_size = os.path.getsize(output_file)\n",
        "    print(f\"\\n✓ File created: {output_file} ({file_size:,} bytes)\")\n",
        "else:\n",
        "    print(f\"\\n✗ File not found at: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Inspect Data\n",
        "\n",
        "Let's load the data into a Spark DataFrame and take a look at it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file from project root's data/ directory\n",
        "data_file = os.path.join(project_root, \"data\", \"redundant_data.csv\")\n",
        "df = spark.read.csv(data_file, header=True, inferSchema=True)\n",
        "\n",
        "# Show basic information\n",
        "print(f\"Total records: {df.count():,}\")\n",
        "print(f\"Columns: {', '.join(df.columns)}\")\n",
        "print(\"\\nFirst 10 records:\")\n",
        "df.show(10, truncate=False)\n",
        "\n",
        "# Show schema\n",
        "print(\"\\nSchema:\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check for Duplicates\n",
        "\n",
        "Let's see how many duplicates we have in our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Count total records\n",
        "total_count = df.count()\n",
        "\n",
        "# Count unique records based on name and email\n",
        "unique_count = df.select(\"name\", \"email\").distinct().count()\n",
        "\n",
        "# Calculate duplicates\n",
        "duplicates = total_count - unique_count\n",
        "duplicate_rate = (duplicates / total_count * 100) if total_count > 0 else 0\n",
        "\n",
        "print(f\"Total records: {total_count:,}\")\n",
        "print(f\"Unique records (by name+email): {unique_count:,}\")\n",
        "print(f\"Duplicate records: {duplicates:,}\")\n",
        "print(f\"Duplicate rate: {duplicate_rate:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have your data loaded, you're ready to start the lab exercises:\n",
        "\n",
        "1. **Exercise 1**: Basic Deduplication - `01_Basic_Deduplication.ipynb`\n",
        "2. **Exercise 2**: Compare Methods - `02_Compare_Methods.ipynb`\n",
        "3. **Exercise 3**: Approximate Methods - `03_Approximate_Methods.ipynb`\n",
        "4. **Exercise 4**: File-Level Deduplication - `04_File_Level_Deduplication.ipynb`\n",
        "\n",
        "## Cleanup\n",
        "\n",
        "When you're done, remember to stop the Spark session:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session when done\n",
        "spark.stop()\n",
        "print(\"✓ Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
