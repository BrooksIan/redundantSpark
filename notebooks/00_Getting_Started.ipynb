{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Deduplication Lab - Getting Started\n",
        "\n",
        "Welcome to the Data Deduplication Lab! This notebook will help you get started with the lab exercises.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "- Understand different deduplication strategies\n",
        "- Be able to process large datasets efficiently with Spark\n",
        "- Know how to use approximate methods for memory-efficient operations\n",
        "- Understand file-level vs record-level deduplication\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Access to Cloudera AI Workbench\n",
        "- Basic Python knowledge\n",
        "- Familiarity with data processing concepts\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's verify that we have access to the necessary files and Spark is available.\n",
        "\n",
        "**Important**: Make sure you have uploaded the following files to your Cloudera AI Workbench project:\n",
        "- `deduplicate_spark.py` (in the project root)\n",
        "- `generate_dataset.py` (in the project root)\n",
        "- `bloom_filter_hyperloglog.py` (in the project root)\n",
        "- `bloom_filter_file_deduplication.py` (in the project root)\n",
        "\n",
        "The notebook will automatically add the project root to the Python path so these modules can be imported.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to Python path so we can import modules\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Try multiple methods to find the project root\n",
        "project_root = None\n",
        "\n",
        "# Method 1: Check if we're in a notebooks subdirectory\n",
        "current_dir = os.getcwd()\n",
        "if 'notebooks' in current_dir:\n",
        "    # Go up one level from notebooks/\n",
        "    project_root = os.path.dirname(current_dir)\n",
        "    print(f\"Method 1: Found project root (from notebooks/): {project_root}\")\n",
        "else:\n",
        "    # Method 2: Look for deduplicate_spark.py in current directory\n",
        "    if os.path.exists('deduplicate_spark.py'):\n",
        "        project_root = current_dir\n",
        "        print(f\"Method 2: Found project root (deduplicate_spark.py in current dir): {project_root}\")\n",
        "    else:\n",
        "        # Method 3: Try going up directories to find deduplicate_spark.py\n",
        "        test_dir = current_dir\n",
        "        for _ in range(3):  # Try up to 3 levels up\n",
        "            if os.path.exists(os.path.join(test_dir, 'deduplicate_spark.py')):\n",
        "                project_root = test_dir\n",
        "                print(f\"Method 3: Found project root (searched up directories): {project_root}\")\n",
        "                break\n",
        "            test_dir = os.path.dirname(test_dir)\n",
        "        \n",
        "        # Method 4: Use current directory as fallback\n",
        "        if project_root is None:\n",
        "            project_root = current_dir\n",
        "            print(f\"Method 4: Using current directory as project root: {project_root}\")\n",
        "\n",
        "# Add project root to path if not already there\n",
        "if project_root and project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "    print(f\"✓ Added to Python path: {project_root}\")\n",
        "\n",
        "# Check if Spark is available\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    print(\"✓ PySpark is available\")\n",
        "except ImportError:\n",
        "    print(\"✗ PySpark not found. Please ensure Spark is installed.\")\n",
        "\n",
        "# Check if our deduplication module is available\n",
        "try:\n",
        "    from deduplicate_spark import create_spark_session\n",
        "    print(\"✓ deduplicate_spark module is available\")\n",
        "except ImportError as e:\n",
        "    print(\"✗ deduplicate_spark module not found.\")\n",
        "    print(f\"  Current working directory: {os.getcwd()}\")\n",
        "    print(f\"  Project root used: {project_root}\")\n",
        "    print(f\"  Python path (first 3): {sys.path[:3]}\")\n",
        "    print(f\"  Error: {e}\")\n",
        "    print(\"\\n  Troubleshooting:\")\n",
        "    print(\"  1. Ensure deduplicate_spark.py is in the project root\")\n",
        "    print(\"  2. Check that the file is uploaded to Cloudera AI Workbench\")\n",
        "    print(\"  3. Verify the file is in the same directory as this notebook's parent\")\n",
        "\n",
        "# Check Python version\n",
        "print(f\"\\n✓ Python version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Spark Session\n",
        "\n",
        "The `create_spark_session()` function automatically detects if you're running in Cloudera AI Workbench and configures Spark appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deduplicate_spark import create_spark_session\n",
        "\n",
        "# Create Spark session (automatically configured for Cloudera)\n",
        "spark = create_spark_session(\"DeduplicationLab\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark master: {spark.sparkContext.master}\")\n",
        "print(\"✓ Spark session created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sample Data\n",
        "\n",
        "Let's generate a sample dataset with duplicates to work with. We'll create a CSV file with 1000 records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Generate sample data\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "# Generate dataset with 1000 records\n",
        "result = subprocess.run(\n",
        "    [\"python\", \"generate_dataset.py\", \"1000\", \"data/redundant_data.csv\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"✓ Sample data generated successfully!\")\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(\"✗ Error generating data:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "# Check if file was created\n",
        "if os.path.exists(\"data/redundant_data.csv\"):\n",
        "    file_size = os.path.getsize(\"data/redundant_data.csv\")\n",
        "    print(f\"\\nFile created: data/redundant_data.csv ({file_size:,} bytes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Inspect Data\n",
        "\n",
        "Let's load the data into a Spark DataFrame and take a look at it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file\n",
        "df = spark.read.csv(\"data/redundant_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show basic information\n",
        "print(f\"Total records: {df.count():,}\")\n",
        "print(f\"Columns: {', '.join(df.columns)}\")\n",
        "print(\"\\nFirst 10 records:\")\n",
        "df.show(10, truncate=False)\n",
        "\n",
        "# Show schema\n",
        "print(\"\\nSchema:\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check for Duplicates\n",
        "\n",
        "Let's see how many duplicates we have in our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Count total records\n",
        "total_count = df.count()\n",
        "\n",
        "# Count unique records based on name and email\n",
        "unique_count = df.select(\"name\", \"email\").distinct().count()\n",
        "\n",
        "# Calculate duplicates\n",
        "duplicates = total_count - unique_count\n",
        "duplicate_rate = (duplicates / total_count * 100) if total_count > 0 else 0\n",
        "\n",
        "print(f\"Total records: {total_count:,}\")\n",
        "print(f\"Unique records (by name+email): {unique_count:,}\")\n",
        "print(f\"Duplicate records: {duplicates:,}\")\n",
        "print(f\"Duplicate rate: {duplicate_rate:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have your data loaded, you're ready to start the lab exercises:\n",
        "\n",
        "1. **Exercise 1**: Basic Deduplication - `01_Basic_Deduplication.ipynb`\n",
        "2. **Exercise 2**: Compare Methods - `02_Compare_Methods.ipynb`\n",
        "3. **Exercise 3**: Approximate Methods - `03_Approximate_Methods.ipynb`\n",
        "4. **Exercise 4**: File-Level Deduplication - `04_File_Level_Deduplication.ipynb`\n",
        "\n",
        "## Cleanup\n",
        "\n",
        "When you're done, remember to stop the Spark session:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session when done\n",
        "spark.stop()\n",
        "print(\"✓ Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
