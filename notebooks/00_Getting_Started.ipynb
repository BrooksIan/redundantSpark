{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Deduplication Lab - Getting Started\n",
        "\n",
        "Welcome to the Data Deduplication Lab! This notebook will help you get started with the lab exercises.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "- Understand different deduplication strategies\n",
        "- Be able to process large datasets efficiently with Spark\n",
        "- Know how to use approximate methods for memory-efficient operations\n",
        "- Understand file-level vs record-level deduplication\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Access to Cloudera AI Workbench\n",
        "- Basic Python knowledge\n",
        "- Familiarity with data processing concepts\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's verify that we have access to the necessary files and Spark is available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Spark is available\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    print(\"✓ PySpark is available\")\n",
        "except ImportError:\n",
        "    print(\"✗ PySpark not found. Please ensure Spark is installed.\")\n",
        "\n",
        "# Check if our deduplication module is available\n",
        "try:\n",
        "    from deduplicate_spark import create_spark_session\n",
        "    print(\"✓ deduplicate_spark module is available\")\n",
        "except ImportError:\n",
        "    print(\"✗ deduplicate_spark module not found. Please upload deduplicate_spark.py to your project.\")\n",
        "\n",
        "# Check Python version\n",
        "import sys\n",
        "print(f\"✓ Python version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Spark Session\n",
        "\n",
        "The `create_spark_session()` function automatically detects if you're running in Cloudera AI Workbench and configures Spark appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deduplicate_spark import create_spark_session\n",
        "\n",
        "# Create Spark session (automatically configured for Cloudera)\n",
        "spark = create_spark_session(\"DeduplicationLab\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark master: {spark.sparkContext.master}\")\n",
        "print(\"✓ Spark session created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sample Data\n",
        "\n",
        "Let's generate a sample dataset with duplicates to work with. We'll create a CSV file with 1000 records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Generate sample data\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "# Generate dataset with 1000 records\n",
        "result = subprocess.run(\n",
        "    [\"python\", \"generate_dataset.py\", \"1000\", \"data/redundant_data.csv\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"✓ Sample data generated successfully!\")\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(\"✗ Error generating data:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "# Check if file was created\n",
        "if os.path.exists(\"data/redundant_data.csv\"):\n",
        "    file_size = os.path.getsize(\"data/redundant_data.csv\")\n",
        "    print(f\"\\nFile created: data/redundant_data.csv ({file_size:,} bytes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Inspect Data\n",
        "\n",
        "Let's load the data into a Spark DataFrame and take a look at it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file\n",
        "df = spark.read.csv(\"data/redundant_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show basic information\n",
        "print(f\"Total records: {df.count():,}\")\n",
        "print(f\"Columns: {', '.join(df.columns)}\")\n",
        "print(\"\\nFirst 10 records:\")\n",
        "df.show(10, truncate=False)\n",
        "\n",
        "# Show schema\n",
        "print(\"\\nSchema:\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check for Duplicates\n",
        "\n",
        "Let's see how many duplicates we have in our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Count total records\n",
        "total_count = df.count()\n",
        "\n",
        "# Count unique records based on name and email\n",
        "unique_count = df.select(\"name\", \"email\").distinct().count()\n",
        "\n",
        "# Calculate duplicates\n",
        "duplicates = total_count - unique_count\n",
        "duplicate_rate = (duplicates / total_count * 100) if total_count > 0 else 0\n",
        "\n",
        "print(f\"Total records: {total_count:,}\")\n",
        "print(f\"Unique records (by name+email): {unique_count:,}\")\n",
        "print(f\"Duplicate records: {duplicates:,}\")\n",
        "print(f\"Duplicate rate: {duplicate_rate:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have your data loaded, you're ready to start the lab exercises:\n",
        "\n",
        "1. **Exercise 1**: Basic Deduplication - `01_Basic_Deduplication.ipynb`\n",
        "2. **Exercise 2**: Compare Methods - `02_Compare_Methods.ipynb`\n",
        "3. **Exercise 3**: Approximate Methods - `03_Approximate_Methods.ipynb`\n",
        "4. **Exercise 4**: File-Level Deduplication - `04_File_Level_Deduplication.ipynb`\n",
        "\n",
        "## Cleanup\n",
        "\n",
        "When you're done, remember to stop the Spark session:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session when done\n",
        "spark.stop()\n",
        "print(\"✓ Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
