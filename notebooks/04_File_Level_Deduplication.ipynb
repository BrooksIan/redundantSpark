{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: File-Level Deduplication\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this exercise, you will:\n",
    "- Learn how to detect duplicate files by content hash\n",
    "- Understand file-level vs record-level deduplication\n",
    "- Calculate potential space savings\n",
    "- See how hash-based file comparison works\n",
    "\n",
    "## What is File-Level Deduplication?\n",
    "\n",
    "**File-level deduplication** is the process of finding duplicate files based on their **content**, not their names or locations. \n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Content-Based Detection**: Two files are considered duplicates if they have the **same content**, even if they have different names or are stored in different locations.\n",
    "\n",
    "2. **Hash Functions**: We use cryptographic hash functions (like MD5 or SHA-256) to create a unique \"fingerprint\" of each file's content. Files with the same hash have identical content.\n",
    "\n",
    "3. **Space Savings**: By identifying duplicate files, you can:\n",
    "   - Delete redundant copies\n",
    "   - Create symbolic links instead of copies\n",
    "   - Optimize storage systems\n",
    "   - Reduce backup storage requirements\n",
    "\n",
    "### File-Level vs Record-Level Deduplication\n",
    "\n",
    "| Aspect | Record-Level | File-Level |\n",
    "|--------|--------------|------------|\n",
    "| **What it finds** | Duplicate rows/records in a dataset | Duplicate files in a storage system |\n",
    "| **Comparison unit** | Individual data records | Entire files |\n",
    "| **Use case** | Cleaning data tables, removing duplicate entries | Storage optimization, backup deduplication |\n",
    "| **Example** | Two rows with same name/email in a CSV | Two files with identical content but different names |\n",
    "| **Method** | Compare column values | Compare file content hashes |\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "- **Backup Systems**: Many backup systems use file-level deduplication to avoid storing the same file multiple times\n",
    "- **Cloud Storage**: Services like Dropbox, Google Drive use deduplication to save storage space\n",
    "- **Version Control**: Git uses content-based addressing (similar concept) to store files efficiently\n",
    "- **Data Lakes**: Large data storage systems deduplicate files to optimize storage costs\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Read each file** and compute its hash (MD5, SHA-256, etc.)\n",
    "2. **Compare hashes** - files with identical hashes have identical content\n",
    "3. **Group duplicates** - identify which files are duplicates of each other\n",
    "4. **Calculate savings** - determine how much space could be saved by removing duplicates\n",
    "\n",
    "### Why Use Hash Functions?\n",
    "\n",
    "- **Fast comparison**: Comparing two hashes is much faster than comparing entire file contents\n",
    "- **Deterministic**: Same content always produces the same hash\n",
    "- **Collision-resistant**: Different content almost never produces the same hash (for good hash functions)\n",
    "- **Fixed size**: Hashes are always the same size regardless of file size (e.g., MD5 = 32 hex chars, SHA-256 = 64 hex chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Find project root\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "elif os.path.exists(os.path.join(current_dir, 'deduplicate_spark.py')):\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    # Search up directories\n",
    "    test_dir = current_dir\n",
    "    for _ in range(5):\n",
    "        if os.path.exists(os.path.join(test_dir, 'deduplicate_spark.py')):\n",
    "            project_root = test_dir\n",
    "            break\n",
    "        parent = os.path.dirname(test_dir)\n",
    "        if parent == test_dir:\n",
    "            break\n",
    "        test_dir = parent\n",
    "    project_root = project_root or current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"✓ Added to Python path: {project_root}\")\n",
    "\n",
    "# Change to project root for file operations\n",
    "os.chdir(project_root)\n",
    "print(f\"✓ Changed working directory to: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deduplicate_spark import create_spark_session, deduplicate_files\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "spark = create_spark_session(\"Exercise4_FileDeduplication\")\n",
    "print(\"✓ Spark session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate or Find Duplicate Files\n",
    "\n",
    "We'll create a set of test files, some of which will be duplicates. The `generate_duplicate_files.py` script creates files with intentional duplicates based on a duplication rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run File-Level Deduplication\n",
    "\n",
    "Now we'll use Spark to:\n",
    "1. Compute a hash (MD5) for each file's content\n",
    "2. Group files by their hash to find duplicates\n",
    "3. Identify which files are unique and which are duplicates\n",
    "4. Calculate potential space savings\n",
    "\n",
    "The `deduplicate_files()` function handles all of this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_files_dir = os.path.join(project_root, \"data\", \"duplicatefiles\")\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "\n",
    "if not os.path.exists(duplicate_files_dir) or len(glob.glob(os.path.join(duplicate_files_dir, \"*\"))) == 0:\n",
    "    print(\"Generating duplicate files...\")\n",
    "    # Create data directory if needed\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"✓ Created data directory: {data_dir}\")\n",
    "    \n",
    "    script_path = os.path.join(project_root, \"generate_duplicate_files.py\")\n",
    "    result = subprocess.run(\n",
    "        [\"python\", script_path, \"25\", \"0.9\", duplicate_files_dir],\n",
    "        cwd=project_root,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Files generated\")\n",
    "    else:\n",
    "        print(f\"✗ Error generating files: {result.stderr}\")\n",
    "else:\n",
    "    print(\"✓ Using existing files\")\n",
    "\n",
    "# Get all files\n",
    "file_paths = glob.glob(os.path.join(duplicate_files_dir, \"*\"))\n",
    "file_paths = [f for f in file_paths if os.path.isfile(f)]\n",
    "print(f\"\\nFound {len(file_paths)} files to analyze\")\n",
    "\n",
    "if not os.path.exists(duplicate_files_dir) or len(glob.glob(os.path.join(duplicate_files_dir, \"*\"))) == 0:\n",
    "    print(\"Generating duplicate files...\")\n",
    "    # Create data directory if needed\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"✓ Created data directory: {data_dir}\")\n",
    "    \n",
    "    script_path = os.path.join(project_root, \"generate_duplicate_files.py\")\n",
    "    result = subprocess.run(\n",
    "        [\"python\", script_path, \"25\", \"0.9\", duplicate_files_dir],\n",
    "        cwd=project_root,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Files generated\")\n",
    "    else:\n",
    "        print(f\"✗ Error generating files: {result.stderr}\")\n",
    "else:\n",
    "    print(\"✓ Using existing files\")\n",
    "\n",
    "# Get all files\n",
    "file_paths = glob.glob(os.path.join(duplicate_files_dir, \"*\"))\n",
    "file_paths = [f for f in file_paths if os.path.isfile(f)]\n",
    "print(f\"\\nFound {len(file_paths)} files to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running file-level deduplication...\")\n",
    "deduplicate_files(spark, file_paths, output_dir=None)\n",
    "print(\"\\n✓ Deduplication complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "After running the deduplication, you should see:\n",
    "- **Total files analyzed**: How many files were checked\n",
    "- **Unique files**: Files with unique content (different hashes)\n",
    "- **Duplicate groups**: Groups of files that share the same content\n",
    "- **Space savings**: How much storage space could be freed by removing duplicates\n",
    "\n",
    "### How to Interpret the Results\n",
    "\n",
    "1. **If two files have the same hash**: They have identical content, even if they have different names\n",
    "2. **Space savings calculation**: If 5 files share the same content, you could keep 1 and delete 4, saving 80% of the space used by those files\n",
    "3. **Hash collisions**: With MD5 or SHA-256, collisions (different content, same hash) are extremely rare in practice\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "1. **How many duplicate files were found?** Look at the duplicate groups count\n",
    "2. **What is the total space that could be saved?** Check the space savings calculation\n",
    "3. **How does file deduplication differ from record deduplication?** \n",
    "   - File-level: Compares entire files using content hashes\n",
    "   - Record-level: Compares individual rows/records in a dataset\n",
    "4. **Why use hash functions instead of comparing files byte-by-byte?**\n",
    "   - Much faster: Hash comparison is O(1) vs O(n) for byte comparison\n",
    "   - Fixed size: Hashes are always the same size regardless of file size\n",
    "   - Efficient: Can quickly identify potential duplicates before detailed comparison\n",
    "5. **What would happen if you had 1000 files, and 100 of them were duplicates of 10 unique files?**\n",
    "   - You'd have 10 unique files\n",
    "   - 90 duplicate files that could be removed\n",
    "   - Significant space savings!\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **File-level deduplication** finds duplicate files by comparing content hashes\n",
    "- **Hash functions** create unique fingerprints of file content\n",
    "- **Space savings** can be significant when many duplicate files exist\n",
    "- **Use cases**: Backup systems, cloud storage, data lakes, storage optimization\n",
    "- **Trade-off**: Hash computation takes time, but enables fast duplicate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"✓ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
