{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Basic Deduplication\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this exercise, you will:\n",
    "- Learn how to remove exact duplicates from a dataset\n",
    "- Understand the `exact` deduplication method\n",
    "- Analyze deduplication results\n",
    "- See where results are saved\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `exact` method is the simplest and fastest deduplication technique. It removes records that are exactly the same based on specified columns (typically `name` and `email`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Find project root\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "elif os.path.exists(os.path.join(current_dir, 'deduplicate_spark.py')):\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    # Search up directories\n",
    "    test_dir = current_dir\n",
    "    for _ in range(5):\n",
    "        if os.path.exists(os.path.join(test_dir, 'deduplicate_spark.py')):\n",
    "            project_root = test_dir\n",
    "            break\n",
    "        parent = os.path.dirname(test_dir)\n",
    "        if parent == test_dir:\n",
    "            break\n",
    "        test_dir = parent\n",
    "    project_root = project_root or current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"✓ Added to Python path: {project_root}\")\n",
    "\n",
    "# Change to project root for file operations\n",
    "os.chdir(project_root)\n",
    "print(f\"✓ Changed working directory to: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deduplicate_spark import create_spark_session, process_file_spark\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session(\"Exercise1_BasicDeduplication\")\n",
    "print(\"✓ Spark session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate or Load Data\n",
    "\n",
    "If you haven't generated data yet, uncomment and run the cell below. Otherwise, we'll use existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Generate data if it doesn't exist\n",
    "data_file = os.path.join(project_root, \"data\", \"exercise1.csv\")\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "    # Create data directory if needed\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"✓ Created data directory: {data_dir}\")\n",
    "    \n",
    "    print(\"Generating sample data...\")\n",
    "    script_path = os.path.join(project_root, \"generate_dataset.py\")\n",
    "    result = subprocess.run(\n",
    "        [\"python\", script_path, \"1000\", data_file],\n",
    "        cwd=project_root,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Data generated successfully\")\n",
    "    else:\n",
    "        print(f\"✗ Error: {result.stderr}\")\n",
    "        print(f\"  Script path: {script_path}\")\n",
    "        print(f\"  Script exists: {os.path.exists(script_path)}\")\n",
    "else:\n",
    "    print(\"✓ Using existing data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Deduplication\n",
    "\n",
    "Now let's run the exact deduplication method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact deduplication\n",
    "stats = process_file_spark(\n",
    "    spark,\n",
    "    os.path.join(project_root, \"data\", \"exercise1.csv\"),\n",
    "    output_dir=None,  # Uses /tmp/results in Cloudera, data/ locally\n",
    "    method='exact'\n",
    ")\n",
    "\n",
    "if stats:\n",
    "    print(f\"\\nOriginal records: {stats['original_count']:,}\")\n",
    "    print(f\"Unique records: {stats['unique_count']:,}\")\n",
    "    print(f\"Duplicates removed: {stats['duplicates_removed']:,}\")\n",
    "    print(f\"Deduplication rate: {stats['deduplication_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to Answer\n",
    "\n",
    "1. How many duplicates were found?\n",
    "2. What percentage of records were duplicates?\n",
    "3. Where are the results saved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"✓ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
