{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Basic Deduplication (Record-Level)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this exercise, you will:\n",
    "- Learn how to remove exact duplicates from a dataset\n",
    "- Understand the `exact` deduplication method\n",
    "- Analyze deduplication results\n",
    "- See where results are saved\n",
    "- Understand record-level vs file-level deduplication\n",
    "\n",
    "## What is Record-Level Deduplication?\n",
    "\n",
    "**Record-level deduplication** is the process of finding and removing duplicate **rows or records** within a dataset. Unlike file-level deduplication (which compares entire files), record-level deduplication compares individual data records.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Record-Based Detection**: Two rows are considered duplicates if they have the **same values** in specified key columns (like `name` and `email`), even if other columns differ.\n",
    "\n",
    "2. **Key Columns**: The columns used to identify duplicates. Common examples:\n",
    "   - `name` + `email` (person identification)\n",
    "   - `product_id` + `sku` (product identification)\n",
    "   - `transaction_id` (unique transaction identifier)\n",
    "\n",
    "3. **Data Quality**: Record-level deduplication improves data quality by:\n",
    "   - Removing accidental duplicate entries\n",
    "   - Cleaning data before analysis\n",
    "   - Reducing storage requirements\n",
    "   - Improving query performance\n",
    "\n",
    "### Record-Level vs File-Level Deduplication\n",
    "\n",
    "| Aspect | Record-Level | File-Level |\n",
    "|--------|--------------|------------|\n",
    "| **What it finds** | Duplicate rows/records in a dataset | Duplicate files in a storage system |\n",
    "| **Comparison unit** | Individual data records (rows) | Entire files |\n",
    "| **Use case** | Cleaning data tables, removing duplicate entries | Storage optimization, backup deduplication |\n",
    "| **Example** | Two rows with same name/email in a CSV | Two files with identical content but different names |\n",
    "| **Method** | Compare column values | Compare file content hashes |\n",
    "| **Granularity** | Row-by-row comparison | File-by-file comparison |\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "- **Customer Databases**: Remove duplicate customer records (same person entered multiple times)\n",
    "- **E-commerce**: Remove duplicate product listings\n",
    "- **Transaction Systems**: Identify and remove duplicate transactions\n",
    "- **Data Warehouses**: Clean data before loading into analytics systems\n",
    "- **ETL Pipelines**: Deduplicate data during extraction/transformation\n",
    "- **Data Lakes**: Remove duplicate records from large datasets\n",
    "\n",
    "### How Record-Level Deduplication Works\n",
    "\n",
    "1. **Identify key columns** - Determine which columns uniquely identify a record (e.g., `name`, `email`)\n",
    "2. **Compare records** - Check if two rows have identical values in the key columns\n",
    "3. **Remove duplicates** - Keep one copy of each unique record, remove the rest\n",
    "4. **Preserve data** - Typically keep the first occurrence or use a strategy (keep latest, keep most complete, etc.)\n",
    "\n",
    "### The `exact` Method\n",
    "\n",
    "The `exact` method is the simplest and fastest deduplication technique:\n",
    "- **How it works**: Compares exact values in specified columns\n",
    "- **Speed**: Very fast - uses Spark's optimized `dropDuplicates()` function\n",
    "- **Use case**: When you want to remove records that are **exactly** the same\n",
    "- **Limitation**: Won't catch near-duplicates (e.g., \"John Smith\" vs \"john smith\" - different case)\n",
    "\n",
    "### Why Deduplicate Records?\n",
    "\n",
    "- **Data Quality**: Duplicate records can skew analysis results\n",
    "- **Storage Efficiency**: Reduces dataset size\n",
    "- **Performance**: Smaller datasets query faster\n",
    "- **Accuracy**: Ensures each entity is counted only once\n",
    "- **Compliance**: Some regulations require clean, deduplicated data\n",
    "\n",
    "### Common Deduplication Strategies\n",
    "\n",
    "1. **Keep First**: Keep the first occurrence, remove subsequent duplicates\n",
    "2. **Keep Last**: Keep the most recent occurrence (useful for time-series data)\n",
    "3. **Keep Most Complete**: Keep the record with the most non-null values\n",
    "4. **Merge**: Combine information from duplicate records into one complete record\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `exact` method is the simplest and fastest deduplication technique. It removes records that are exactly the same based on specified columns (typically `name` and `email`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Find project root\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "elif os.path.exists(os.path.join(current_dir, 'deduplicate_spark.py')):\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    # Search up directories\n",
    "    test_dir = current_dir\n",
    "    for _ in range(5):\n",
    "        if os.path.exists(os.path.join(test_dir, 'deduplicate_spark.py')):\n",
    "            project_root = test_dir\n",
    "            break\n",
    "        parent = os.path.dirname(test_dir)\n",
    "        if parent == test_dir:\n",
    "            break\n",
    "        test_dir = parent\n",
    "    project_root = project_root or current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"✓ Added to Python path: {project_root}\")\n",
    "\n",
    "# Change to project root for file operations\n",
    "os.chdir(project_root)\n",
    "print(f\"✓ Changed working directory to: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deduplicate_spark import create_spark_session, process_file_spark\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session(\"Exercise1_BasicDeduplication\")\n",
    "print(\"✓ Spark session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate or Load Data\n",
    "\n",
    "If you haven't generated data yet, uncomment and run the cell below. Otherwise, we'll use existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Generate data if it doesn't exist\n",
    "data_file = os.path.join(project_root, \"data\", \"exercise1.csv\")\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "    # Create data directory if needed\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"✓ Created data directory: {data_dir}\")\n",
    "    \n",
    "    print(\"Generating sample data...\")\n",
    "    script_path = os.path.join(project_root, \"generate_dataset.py\")\n",
    "    result = subprocess.run(\n",
    "        [\"python\", script_path, \"1000\", data_file],\n",
    "        cwd=project_root,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Data generated successfully\")\n",
    "    else:\n",
    "        print(f\"✗ Error: {result.stderr}\")\n",
    "        print(f\"  Script path: {script_path}\")\n",
    "        print(f\"  Script exists: {os.path.exists(script_path)}\")\n",
    "else:\n",
    "    print(\"✓ Using existing data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Deduplication\n",
    "\n",
    "Now let's run the exact deduplication method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact deduplication\n",
    "stats = process_file_spark(\n",
    "    spark,\n",
    "    os.path.join(project_root, \"data\", \"exercise1.csv\"),\n",
    "    output_dir=None,  # Uses /tmp/results in Cloudera, data/ locally\n",
    "    method='exact'\n",
    ")\n",
    "\n",
    "if stats:\n",
    "    print(f\"\\nOriginal records: {stats['original_count']:,}\")\n",
    "    print(f\"Unique records: {stats['unique_count']:,}\")\n",
    "    print(f\"Duplicates removed: {stats['duplicates_removed']:,}\")\n",
    "    print(f\"Deduplication rate: {stats['deduplication_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "After running the deduplication, you should see:\n",
    "- **Original count**: Total number of records in the input file\n",
    "- **Unique count**: Number of unique records after deduplication\n",
    "- **Duplicates removed**: How many duplicate records were found and removed\n",
    "- **Deduplication rate**: Percentage of records that were duplicates\n",
    "\n",
    "### How to Interpret the Results\n",
    "\n",
    "1. **Deduplication rate**: If you see 25%, that means 1 in 4 records was a duplicate\n",
    "2. **Unique records**: These are the records you'll keep - one for each unique combination of key columns\n",
    "3. **Output location**: Results are saved to `/tmp/results/` in Cloudera (or `data/` locally) as a Parquet file\n",
    "\n",
    "### Example Scenario\n",
    "\n",
    "If you have 1000 records and the deduplication shows:\n",
    "- Original: 1,000 records\n",
    "- Unique: 750 records\n",
    "- Removed: 250 duplicates\n",
    "- Rate: 25%\n",
    "\n",
    "This means:\n",
    "- 250 records were exact duplicates of other records\n",
    "- You now have 750 unique records\n",
    "- You saved 25% of storage space (if storing the deduplicated version)\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "1. **How many duplicates were found?** Look at the \"Duplicates removed\" value\n",
    "2. **What percentage of records were duplicates?** Check the deduplication rate\n",
    "3. **Where are the results saved?** The output path will be shown in the results\n",
    "4. **Why might records be duplicated?** Common causes:\n",
    "   - Data entry errors (same person entered twice)\n",
    "   - Multiple data sources merged without deduplication\n",
    "   - System errors creating duplicate transactions\n",
    "   - Data migration issues\n",
    "5. **What happens if two records have the same name but different emails?** \n",
    "   - They are **NOT** considered duplicates (different key column values)\n",
    "   - Only records with identical values in ALL key columns are duplicates\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Record-level deduplication** removes duplicate rows from datasets\n",
    "- **Key columns** determine what makes a record unique\n",
    "- **Exact method** is fastest but only catches exact duplicates\n",
    "- **Use cases**: Data cleaning, quality improvement, storage optimization\n",
    "- **Trade-off**: Fast exact deduplication vs slower fuzzy matching (covered in later exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"✓ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
